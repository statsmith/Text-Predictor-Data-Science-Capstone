---
title: "Data Science Capstone Milestone Report"
author: "David Smith"
date: "Saturday, November 08, 2014"
output: html_document
---

```{r Initialize, cache=TRUE, echo=FALSE}

# Options
        options(scipen=999)

# Libraries
        library(tm)
        library(qdap)
        library(RWeka)
        library(openNLP)

# Set Constants
        setwd("C:/Users/HFDSS103/Documents/Coursera/Capstone/Corpus25KNoStem")

        
# Functions
        f1 <- content_transformer(function(x, replaceMe, withMe) gsub(replaceMe, withMe, x))
        BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
        TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
        

# Get Corpus
        myCorpus <- Corpus(DirSource(encoding="UTF-8") )

```

```{r Tokenize, cache=TRUE, echo=FALSE}
# Tokenize
        # Rweka Word Token
        myWordToken <- WordTokenizer(myCorpus)
        
        # Rweka 2-gram Token
        tdm2gram <- TermDocumentMatrix(myCorpus, control = list(tokenize = BigramTokenizer))
        
        # RWeka 3-gram Token
        tdm3gram <- TermDocumentMatrix(myCorpus, control = list(tokenize = TrigramTokenizer))
        
```


### Background ###

```{r docLines, echo=FALSE, cache=TRUE}
# How Many Lines in Each Text Document: length function not working in markdown... fix this!
        lBlogs <- length(myCorpus[[1]]$content)
                lBlogs <- 64517
        lNews <- length(myCorpus[[2]]$content)
                lNews <- 56700
        lTweets <- length(myCorpus[[3]]$content)
                lTweets <- 28039
        lTotal <- lBlogs + lNews + lTweets

```


In partnership with [SwiftKey](http://swiftkey.com/en/), the Coursera Data Science Capstone project challenges students to apply what they have learned in the data science certification program, as well as their independence, creativity, and initiative in the development of a data product highlighting predictive text models.  (I bet you could have predicted that last word "models"...).  

The [texts](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) used for this project include US blogs, news, and tweets. Note that prior to the exploratory analysis, the texts were cleaned by converting everything to lowercase, removing extra white spaces, removing special characters, removing offensive words, converting all end-of-sentence characters to periods, converting all numbers to the symbol "#", and separating the data into sentences.  In addition, only the first 25000 lines of each text source were included for exploratory analysis, yielding a total of `r lTotal` lines for analysis (`r lBlogs` from blogs, `r lNews` from news, and `r lTweets` from tweets).


### Objectives ###

The objectives of this milestone report are to...  

* Demonstrate the data has been successfully downloaded  
* Provide summary statistics about the data sets  
* Report interesting findings  
* Get feedback on plans for creating a prediction algorithm   





### Distribution of Word Frequencies ###

The first step in exploring the data is to simply identify the distribution of word frequencies.  


```{r Explore1, echo=FALSE, results='hide', cache=TRUE}

# Distribution of Word Frequency

        dfWords <- data.frame(table(myWordToken))
        dfWords <- dfWords[order(-dfWords$Freq),]
        names(dfWords) <- c("Word","Freq")

        totalWords <- sum(dfWords$Freq)
        uniqueWords <- dim(dfWords)[1]
        singleWords <- dim(dfWords[dfWords$Freq==1,])[1]
        singleRatio <- round(100*singleWords/uniqueWords,1)

        i=145  # 145 Words covers 50% of all Words
        sum(dfWords$Freq[1:i])/totalWords 

        j=8500  # 145 Words covers 50% of all Words
        sum(dfWords$Freq[1:j])/totalWords 

       

```

* There are `r totalWords` total "words" in the corpus
* There are `r uniqueWords` unique words in the corpus
* Therefore, `r singleWords` words (`r singleRatio`%) are used only once in the corpus (and therefore may not be very useful for prediction purposes)  

Figure 1 below plots the Log10 frequency of words in the corpus.  Note that the distribution is highly skewed, further suggesting that...  

* A large number of words are only included once (recall: Log10(1)=0)  
* A relatively small proportion of the words are used extremely often (and therefore represent a majority of the text).   

```{r Explore2, echo=FALSE, cache=TRUE}

# Distribution of Word Frequency

        hist(log10(dfWords$Freq),col="blue", xlab="Log10 (Word Frequency)", ylab="Number of Words", main="Figure 1: Distribution of Word Frequencies") 
        
```

  
Figure 2 (below) further summarizes the idea that a relatively small number of unique words account for a large proportion of all the words in the corpus.  In fact, about `r i` words account for 50% of all the words in the corpus.  Interestingly, we need about `r j` words to account for 90% of all words in the corpus.


```{r Explore4, echo=FALSE, cache=TRUE}

    
         # Word Coverage...
                goTo <- 2000
                m <- matrix(NA, nrow=goTo, ncol=2)
                m[ ,1] <- 1:goTo
                for (i in 1:goTo){
                        m[i,2] <- sum(dfWords$Freq[1:i])/totalWords
                }

        plot(m[,2]~m[,1],type="l", xlab="Number of Unique Words", 
             ylab="Proportion of Total Words in Corpus", 
             main="Figure 2: Word Coverage", col="blue") + 
                abline(v=145, col="red", lty=2) + abline(h=0.5, col="red", lty=2)


```


### Most Frequent Words ###

The next logical question is "which words are used most frequently"?  The table below summarizes the top 20 most frequent words.  Notice that these frequent words tend to be conjunctions, pronouns, prepositions, or conjugations of the verb "to be", with a small number of letters.  


```{r Explore3, echo=FALSE, cache=TRUE}

# Distribution of Word Frequency

        print.data.frame(dfWords[1:20,],row.names=FALSE)

```


### Word Combinations ###

Next, we consider the most frequent two and three word combinations (i.e. 2-grams and 3-grams).  As with individual words, we can quantify the number and frequency of word combinations.


```{r Explore5, echo=FALSE, cache=TRUE}

print("hi there")
        m.tdm2gram <- as.matrix(tdm2gram)

        head(m.tdm2gram)
        class(m.tdm2gram)
       
        
       

```







Figure 1 below plots the Log10 frequency of words in the corpus.  Note that the distribution is highly skewed, further suggesting that...  

* A large number of words are only included once (recall: Log10(1)=0)  
* A relatively small proportion of the words are used extremely often (and therefore represent a majority of the text).   





